---
title: "Replication Report for Experiments 1 and 5 by ter Wal et al. (2021, Nature Communications)"
author: "Alice Xue (alicexue@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

How does the hippocampus - a subcortical brain structure - support episodic memory encoding and retrieval? I am interested in studying how theta oscillations in the hippocampus play a role in these memory processes. Computational models of information processing in the hippocampus make predictions about when encoding and retrieval may take place optimally with respect to different phases of the hippocampal theta rhythm. These rhythmic neural processes may have downstream consequences on behavior, a prediction tested by ter Wal et al. (2021). In this paper, the authors assessed whether the timing of behavioral responses is rhythmic in memory-dependent vs. memory-independent (visual) tasks. In addition, they looked at theta rhythmicity in intracranial EEG recordings, which will be ignored in the current behavioral replication study.

### Justification for choice of study

I am interested in rhythms in cognition, and am curious about the reliability of the interesting behavioral effects reported in this paper. By attempting to replicate this study, I will learn more about the analysis approach the authors developed; this will be useful for understanding different ways of studying behavioral/neural rhythms and may be useful for my own research. 

### Anticipated challenges

I expect the data analysis portion to be the main challenge for me in conducting this replication project, as the computation of the Oscillation score appears to be rather complex. I am planning to rewrite the authors' Matlab code (provided on GitHub) in R, a language I am not yet particularly comfortable programming in. 

### Links

Project repository: https://github.com/psych251/wal2021.git

Original paper: https://github.com/psych251/wal2021/blob/main/original_paper/Wal2021_NatComms.pdf

Study paradigm for the memory-dependent task: https://walrep.pythonanywhere.com/MEM

Study paradigm for the memory-independent task: https://walrep.pythonanywhere.com/MEM/vis

## Methods

### Description of the steps required to replicate the results

I will need to identify and collect the word and picture stimuli that the authors used for the behavioral tasks in their paper.

In the memory-dependent study (Experiment 5), participants take part in an encoding phase, a distractor phase, and a retrieval phase in multiple blocks. In the encoding phase, participants are instructed to create a vivid mental image that involves a word cue and an object (each stimulus is presented at different times; 128 associations total), and indicate with a button press when they have successfully encoded the association. In the distractor phase, participants judge whether numbers displayed sequentially on the screen are odd or even. In the retrieval (with-catch) phase, they are presented a semantic/perceptual question (“animate or inanimate?” / “photo or drawing?”), followed a word probe. Participants are instructed to recall the object they previously imagined with the word probe, and report using button presses whether the object was animate/inanimate or presented as a photo/drawing. They can also report if they forgot the associated object. 

In the memory-independent (visual) study (Experiment 1), participants are first presented with a semantic/perceptual question (“animate or inanimate?” / “photo or drawing?”). This is followed by the presentation of an object, during which time the participants provide their response with a button press.

For each task (encoding, retrieval, & visual), I will compute an Oscillation score (O-score) using the procedures described in the paper and as implemented in the Matlab code the authors provided on GitHub. I will rewrite their Matlab code in R to better understand the analysis procedure and become more comfortable programming in this language.

*O-score computation:* This procedure is described in great detail in the paper. Briefly, the reaction time distributions for correct responses are first subject to an auto-correlation and then smoothed with a Gaussian kernel. The central peak of the resulting auto-correlation histogram is removed, and the positive lags on the histogram are Fourier transformed. The O-score is defined as the magnitude of the frequency with the highest peak (within an experimenter-specified range) divided by the average magnitude of the frequency spectrum. The O-scores are then z-scored against 500 O-scores computed on shuffled data generated using participant-specific response trends.

### Power Analysis

The effect sizes for various O-score comparisons between tasks were provided in the supplement of the paper. The Cohen's d of interest (between memory-dependent and memory-independent tasks; i.e., encoding/retrieval/catch with retrieval vs. visual) ranged from 0.86 to 0.92. According to the software G\*Power, a more conservative estimate of the total number of subjects needed to achieve 80% power, using the lower bound of this effect size range, is 46 (23 subjects per group). Since I decided to reproduce some of the original findings to familiarize myself with the analysis procedure and R programming more generally, I calculated the effect size myself using data from just two of the original experiments. My key analysis of interest, as described below, is a linear mixed effects model. This model didn't converge with data from these two experiments. I therefore calculated the effect size using t-tests as well as a simple linear model. The effect size for the two-sided t-test comparing memory-dependent (encoding + retrieval) and memory independent tasks (visual) was 0.74 (results and calculations shown below). For this effect size, G\*Power suggested that 60 participants are needed to achieve 80% power. For the linear model, the partial Cohen's f2 was 0.15 and the power analysis indicated that 55 participants are needed to achieve 80% power. 

The original paper had five versions of the memory-dependent study and four versions of the memory-independent study. It is possible that the effect size is sensitive to subtle differences in the experimental design, but it seems unlikely that the effects should differ drastically between experiments. I ultimately decided to collect data from 50 participants (25 per experiment). Assuming that the effects hold in the experiments I selected (both of which use "Standard" stimuli), a sample of this size should be sufficient to detect an effect of memory dependence on the extent to which behavior in a given task is oscillatory.

### Planned Sample

My goal is to collect 25 participants for the memory-dependent experiment and 25 participants for the memory-independent experiment. 

### Materials

Materials include 128 action verbs and 128 object images. Thank you to the authors for providing the word and image stimuli used in the original experiments.

### Procedure	

#### Memory-dependent experiment
- Encoding phase: A fixation cross will first be shown for 0.5-1.5s. This will be followed by the display of a cue (action verb) for 2s. After another fixation display (0.5-1.5s), an object will be presented for at most 7s, during which time participants should respond when they have successfully made an association between the cue and the stimulus. These trials will be administered in blocks of 8 trials. Participants will learn 128 associations.
- Distractor phase: For one minute, participants will indicate whether numbers presented serially on the display are odd or even.
- Retrieval with catch phase: On each trial, participants will be presented a perceptual or semantic question (drawing or photo; animate or inanimate; forgotten) for 3s. A cue is then presented for a maximum of 7s, and participants will respond to the initial question with a button press. Each object will be probed with a perceptual question and a semantic question, in a pseudorandom order such that the same cue is shown at least two trials apart. Trials will be separated by a presentation of a fixation cross for 0.5-1.5s. Each block contains 16 trials. Feedback is provided after the distractor and retrieval phases (% accuracy).

#### Visual experiment

A fixation cross will be displayed for 0.5-1.5s. This will be followed by the presentation of a perceptual or semantic question (drawing or photo; animate or inanimate) for 3s. An object will then be displayed for up to 3s, during which time participants are to provide a response to the question. The 128 stimuli will be displayed twice, such that participants provide an answer to both the perceptual and semantic questions. Trial order will be randomized and trials will be administered in blocks of 32 trials. Feedback is provided after each block.

### Analysis Plan

Participants will be included if the accuracy of responses to catch questions is above chance level (using a one-sided binomial test with a guessing rate of 50%), and at least 10 correct button presses were made. Trials where reaction times are longer or shorter than 2.5 standard deviations away from the task and participant-specific mean will be excluded. To compute the O-score of interest, I will follow the procedure outlined in the Methods section.   

**Clarify key analysis of interest here**  

In the key analysis, I will use the following linear mixed-effects model: `o_score_z ~ memory_dependence + length(time_series) + (1|subject)`, where `o_score_z` is the z-scored O-score and `length(time_series)` is the difference between the last and first reaction time. If this model does not converge, I will run a regular linear model or a t-test to compare O-scores in the memory-dependent and memory-independent tasks.

### Differences from original study

The original study had several experiments. My goal is to replicate two of the behavioral experiments (as my key analysis depends on a comparison between a memory-based task and a visual control task that they ran in two separate samples of participants). A major difference is that all of the original experiments were conducted in person, whereas mine will be conducted online (on Prolific). I anticipate that online participants may be less attentive and more likely to rush through the study; this could affect reaction times, a key dependent variable in the analyses. 

There are additionally differences in the software being used to run the experiments. Whereas the original authors used PsychToolBox, I will be using jsPsych to present each task. Since participants will be using their own devices and different browsers to take part in the study, it is possible that there will be slight differences in stimulus presentation timing and response time precision, but I don't expect these differences to affect the main results. Although response timing is a critical dependent variable in these studies, reaction times can be as long as 7 seconds, and discrepancies in reaction times due to software should be on the order of tens of milliseconds at most. The online tasks will also have a progress bar at the top of the screen indicating how far along participants are in the study.

On the analysis side, my code will be in R, whereas the original authors used Matlab. The original authors used fitlme in Matlab to run their linear mixed effects model, and I will be using the lme4 package in R. It is possible that this difference in software will impact the results.

Since the original set of experiments had a couple hundred participants in total, I was concerned that my study would be quite underpowered in comparison given my limited resources. However, I conducted a power analysis using the lower bound of the effect size of interest that the authors reported, and it appears that I should have sufficient power to detect a difference in O-scores between memory-dependent and memory-independent tasks (assuming that all 50 participants perform well enough to be included in the final sample). In the original paper, it is unclear if the effect size differs depending on which variants of the experiments are selected (the ones I selected use "Standard" stimuli). The effect size I calculated using my reproduced results suggest I may need a slightly larger sample to detect an effect. 

Since I will be replicating this study in an online sample, I plan to reduce the overall length of each experiment. In memory-dependent study, participants will encode 96 associations as opposed to 128 associations. In the retrieval task, participants see each object twice, resulting in 192 retrieval trials as opposed to 256. In addition, the distractor task will be 20s long instead of 60s long. I am also reducing the duration of the memory-independent study such that participants see each of the 128 stimuli once (just one perceptual or one semantic question) as opposed to twice (both questions). In the visual task, there will be 4 blocks of 32 trials.

Finally, as in the original studies, there will be practice trials in both experiments. Since my replication study will be run in an online sample, I will include an instructions quiz for the encoding and retrieval phases of the memory-dependent task before the practice trials, as well as an instructions quiz before the visual task's practice trials. Participants will have to answer every quiz question correctly before they can continue with the study. These quizzes will be included to ensure that participants understand the task; their inclusion should not impact the main results.

### Methods Addendum (Post Data Collection)

```{r setup, include=FALSE}
library(pracma)
library(testit)
library(gsignal)
library(matlab)
library(stats)
library(dplR)
library(fitdistrplus)
library(ggplot2)
library(tidyverse)
library(sensemakr)
library(data.table)
library(report)
library(knitr)
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)
```


#### Actual Sample
Fifty participants were recruited to take part in this study: 25 participants performed the memory-dependent tasks and 25 participants performed the memory-independent task. 
As shown below, all participants' data were analyzed. Three participants' encoding data were excluded for having too few data points, as specified in the analysis plan.

##### Demographics:
```{r demographics, echo=F}
demo <- 
    list.files(path = "./wal_online_task", pattern = "*_DemographicQuestionnaire.csv", recursive=T, full.names=T) %>% 
    map_df(~fread(.))

kable(demo %>%
  filter(QuestionNum == 1) %>%
  group_by(Question) %>%
  mutate(Answer = as.integer(Answer)) %>%
  summarize(mean_age = mean(Answer, na.rm=T), sd_age = sd(Answer, na.rm=T)) %>%
  select(mean_age, sd_age),
  col.names = c("Mean age","SD age"),
  align='l')

kable(demo %>%
  filter(QuestionNum == 2) %>%
  mutate_if(is.character, ~na_if(., '')) %>%
  mutate(Answer = replace_na(Answer, "Did not respond")) %>%
  group_by(Question) %>%
  count(Answer) %>%
  ungroup() %>%
  select(Answer, n),
  col.names = c("Sex","N participants"),
  align='l')

kable(demo %>%
  filter(QuestionNum == 3) %>%
  group_by(Question) %>%
  count(Answer) %>%
  ungroup() %>%
  select(Answer, n),
  col.names = c("Race/ethnicity","N participants"),
  align='l')
```

#### Differences from pre-data collection methods plan
None

## Results

### Data preparation

Data preparation following the analysis plan.
	
#### Functions for O-score computation

I rewrote the original Matlab code provided by the authors on GitHub: https://github.com/marijeterwal/behavioral-oscillations

```{r internal_functions, echo=T}
# Code rewritten from Matlab: https://github.com/marijeterwal/behavioral-oscillations

# DESCRIPTION:
# Take the discrete events at the time points in data and converts them to a 
# (smoothed) continuous time series.
# 
# INPUTS:
# - cfg: struct with settings (see below)
# - data: vector with time points
# 
# OUTPUTS:
# - signal: time series
# - tspan: time axis corresponding to signal
# 
# CONFIG:
# - dt: time step for continuous trace (required)
# - sd_smooth: standard deviation for gaussian smoothing kernel (optional)
# - width_block: width of block smoothing kernel (optional)
# - removeVal: values to be removed from the data (optional)
# - quantlim: extremes to be removed from the data (fractions, optional) 
# ([qmin, qmax])
# - warnings: switch for warnings (true or false)
make_continuous_trace <- function(data, dt, sd_smooth, width_block, remove_val, quantlim, warnings) {
  warnings = FALSE
  if (missing(dt)) {
    stop("Time step is required") 
  }
  if (!missing(sd_smooth)) {
    if (is_empty(sd_smooth)) {
      wind = rep(0, 3)
      wind[2] = 1
    } else {
      sdwind <- round(sd_smooth / dt)
      gt <- 1:8*sdwind
      # in matlab, 
      # y = normpdf(x,mu,sigma) 
      # returns the pdf of the normal distribution with mean mu and standard deviation sigma, 
      # evaluated at the values in x.
      wind <- dnorm(gt, 4*sdwind, sdwind)
    }
  } else if (!missing(width_block)) {
    wind <- rep(1, round(width_block / dt)) 
  } else {
    wind <- rep(0, 3)
    wind[2] <- 1 
  }
   
  tspan = seq(from = 0, to = max(data)+dt, by = dt)
  
  # make time series of responses
  dum <- rep(0, length(tspan))
  id_saved <- rep(0, length(tspan))
  data_r <- rep(0, length(tspan))
  r_vals <- rep(0, length(tspan))
  for (r in 1:length(data)) {
    continue = TRUE
    if (is.nan(data[r]) || (!missing(remove_val) && abs(data[r]-remove_val) <= dt)) {
      continue = FALSE
    }
    
    if (continue) {
      id <- which(as.integer(round(tspan*(1/dt))) == as.integer(round(data[r]*(1/dt)))) ## note that in matlab, int32 rounds the input before converting to an integer, whereas as.integer in r just truncates the input
       
      data_r[id] <- data[r]
      r_vals[id] <- r
     
      # if not found
      if (length(id)==0) {
        print(r)
      } else {
        id_saved[id] <- id
      }
      dum[id] <- dum[id] + 1
    }
  }
  
  if (!missing(quantlim) && !is_empty(quantlim)) {
    q <- as.integer(round(quantile(which(dum>0), quantlim, type = 5))) # type = 5 matches the algorithm used in matlab
    dum <- dum[q[1]:q[2]]
    tspan <- tspan[q[1]:q[2]]
  }

  if (sum(dum) != length(data) && warnings==TRUE) {
    print('Datapoints missing')
  }
  
  signal <- gsignal::wconv('1d', dum, wind, shape='same')
  
  return(list("signal" = signal, "tspan" = tspan))
}


# ----- spectralPeak -----
# 
# DESCRIPTION:
# Computes the spectrum using the Fast Fourier Transform and finds its peak
# 
# INPUTS:
# - cfg: struct with settings
# - data: time series
# 
# OUTPUTS:
# - freq: frequency belonging to the spectral peak
# - fxx: frequency axis
# - corfft: spectrum matching fxx
# 
# CONFIG:
# - fs: sampling rate (required)
# - flim: frequency bounds within which the peak should be found ([fmin,
# fmax])
# - fcor: switch to use log fit to remove 1/f trend (boolean)
# - taper: taper that is applied to the data before fft.
spectral_peak <- function(fs, flim, fcor, taper, data) {
  if (missing(fs)) {
    stop("Sampling rate not specified")
  }
  
  if (missing(flim)) flim <- list()
  if (missing(fcor)) fcor <- FALSE
  if (missing(taper)) taper <- 'none'
  
  ## compute the fft on data
  L <- length(data)
  if (taper == 'none') {
    tmp <- fft(data)
  } else { 
        tmp <- fft(eval(parse(text=paste('signal::',taper,'(n=L)', sep='')))*data)
  }
  
  ps <- abs(tmp[1:round(L/2)]/L)
  ps[2:(length(ps)-1)] <- 2*ps[2:(length(ps)-1)]
  
  fx <- linspace(0, fs/2, round(L/2))
  if (missing(flim)) {
    fxx <- fx
    datfft <- ps
  } else {
    f1 <- which(fx>=flim[1])[1]
    if (f1<1) f1=1
    f2 <- which(fx>=flim[2])[1]-1
    if (length(f2)==0) f2 = length(fx)
    fxx <- fx[f1:f2]
    datfft <- ps[f1:f2]
  }
  
  ## 1/f correction
  if (fcor) {
    # fit 1/f
    print("This 1/f fitting is very basic, proceed with caution...")
    p <- polyfit(log10(fxx), reshape(log10(datfft), size(fxx)), 1)
    
    # correct for 1/f
    corfft <- datfft - reshape(10^(log10(fxx)*p[1]+p[2]), size(datfft))
  } else {
    corfft <- datfft
  }
  
  ## find peak
  if (length(corfft) < 3) { # can't determine a peak if data is too short
    freq <- NaN
  } else {
    # find the highest peak
    highest_peak <- pracma::findpeaks(x=as.double(corfft), npeaks=1, sortstr=TRUE)
    highest_peak_x <- highest_peak[2]
    freq <- fxx[highest_peak_x]
  }
  
  return(list("freq" = freq, "fxx" = fxx, "corfft" = corfft))
}


# ----- oscillationScore -----
# computes the correlation score as published in:
# Muresan et al., J Neurophysiol, 2008
# for binary data (spike trains, reaction times)
# and continuous data (LFPs, ...)
# 
# INPUTS: 
# - fs in s^-1
# - flim: [fmin, fmax] in Hz
# - signal: time series with sampling rate fs
# can also specify the following
# - mincycles
# - quantile of data to use (only for spikes/RTs!)
#     quantlim: [quantile_min quantile_max] values between 0 and 1 (default: [])
# - boolean to determine whether ACH has to be smoothed (might not be necessary for continuous signals)
#     smoothach: true or false (default: true)
# - width of fast smoothing kernel
#     smoothwind: s (default: determined based on flim)
# - width of fast smoothing kernel
#      peakwind: s (default: determined based on flim)
# - angular threshold for peak detection
#      thresangle in degrees: double (default: 10)
# - minimum width of frequency band:
#      minfreqbandwidth: double (default: no minimum)
# - frequency of interest:
#      fpeak: Hz (default: [])
# - plot: true or false (default: false)
# - warnings: 'on' or 'off' (default: on)
# 
# OUTPUTS
# - oscore: Oscillation score
# - fosc: Frequency corresponding to Oscillation score
# - flim: Frequency band used for the analysis
# - flimfft: Spectrum within the frequency bounds
# - freqs: frequency axis corresponding to flimfft
oscillation_score <- function(fs, cfg_flim, signal, quantlim=list(), smoothach=TRUE, mincycles, smoothwind, peakwind, thresangle=10, minfreqbandwidth, fpeak=list(), plot=FALSE, warnings=TRUE, taper, fcor) {
  
  assert("Invalid sampling rate", fs > 0)
  assert("Invalid frequency range of interest", is.vector(cfg_flim) && length(cfg_flim) == 2)
  
  if (missing(mincycles)) mincycles <- 3
  if (missing(thresangle)) thresangle <- 10
  
  if (missing(smoothach)) smoothach = TRUE
  if (missing(smoothwind)) smoothwind = list()
  if (missing(peakwind)) peakwind = list()
  
  if (missing(quantlim)) quantlim = list()
  if (missing(minfreqbandwidth)) minfreqbandwidth = list()
  if (missing(fpeak)) fpeak = list()
  
  if (missing(plot)) plot = FALSE
  if (missing(warnings)) warnings = TRUE
  
  ## check signal
  
  # checking signal quality
  if (sum(signal) < 3) {
    if (warnings) {
      print("Dataset is (nearly) empty: Oscillation score could not be computed")
    }
    
    oscore <- NA
    fosc <- NA
    flim <- list()
    flimfft <- list()
    freqs <- list()
    return(list("oscore"=oscore, "fosc"=fosc, "flim"=flim, "flimfft"=flimfft, "freqs"=freqs))
  }

  # exclude 'outliers' in discrete data
  if (length(quantlim)>0) {
    q <- as.integer(round(quantile(which(signal>0), quantlim, type = 5)))
    signq <- signal[max(1,q[1]-1):min(q[2]+1,length(signal))]
  } else {
    # make sure the data do not contain long empty stretches at beginning and end
    tstart <- max(1, which(signal>0)[1]-1) 
    tend <- min(length(signal), tail(which(signal>0),1)+1)
    signq <- signal[tstart:tend] 
  }
  
  # check fmin and fmax
  fmin <- max(cfg_flim[1], mincycles*fs/length(signq))
  fmax <- min(cfg_flim[2], sum(signq)/(length(signq)/fs))
  flim <- c(fmin, fmax)
  
  # find the desired width of the ACH: w
  w = 2^(1+floor(max(log2(2*mincycles*fs/cfg_flim[1]), log2(fs/2))))
  
  # find s_fast and s_slow
  if (is_empty(smoothwind)) {
    smoothwind <- min(0.002, 134/(1.5 * cfg_flim[2]) / 1000) # s - following Muresan et al., 2008
  }
  
  if (is_empty(peakwind)) {
    peakwind <- 2*134/(1.5*cfg_flim[1]) / 1000 # s
  }
  
  if (length(minfreqbandwidth) != 0 && (fmax-fmin) < minfreqbandwidth) {
    if (warnings) {
      print("Data not sufficient to meet the minimal frequency bandwidth.")
    }
    
    oscore <- NA
    fosc <- NA
    flim <- list()
    flimfft <- list()
    freqs <- list()
    return(list("oscore"=oscore, "fosc"=fosc, "flim"=flim, "flimfft"=flimfft, "freqs"=freqs))
  }
  
  ## compute oscillation score
  
  # STEP 1: Autocorrelation
  ach <- gsignal::xcorr(signq)$R
  
  # STEP 2: smooth AC
  if (smoothach) {
    sdwind <- round(smoothwind * fs)
    # gaussian
    gt <- (-4*sdwind):(4*sdwind)
    wind <- 1/(sdwind*sqrt(2*pi)) * exp(-1*(gt^2 / (2*sdwind^2)))
    ach_smooth <- gsignal::wconv('1d', ach, wind, 'same') # missing same parameter
  } else {
    ach_smooth <- ach
  }
  
  # STEP 3: remove peak
  sdwind <- round(peakwind * fs)
  gt <- (-4*sdwind):(4*sdwind)
  wind = 1/(sdwind*sqrt(2*pi)) * exp(-1*(gt^2 / (2*sdwind^2)))
  ach_slow = gsignal::wconv('1d', ach, wind, 'same')
  thres = tan(pi*(thresangle)/180);
  scalfact = (length(ach)-1) / ach_slow[ceil(length(ach)/2)]
  
  if (which(size(ach_slow)>2) == 1) { # column array
    diff_ach <- flipud(diff(ach_slow[1:ceil(length(ach)/2)]))
  } else if (which(size(ach_slow)>2) == 2) { # row array
    diff_ach <- fliplr(diff(ach_slow[1:ceil(length(ach)/2)]))
  } else {
    stop("Signal has the wrong size, consider squeezing!")
  }
  
  if (is_empty(diff_ach)) {
    if (warnings) {
      print("Dataset is (nearly) empty: Oscillation score could not be computed.")
    }
    oscore <- NA
    fosc <- NA
    flim <- list()
    flimfft <- list()
    freqs <- list()
    return(list("oscore"=oscore, "fosc"=fosc, "flim"=flim, "flimfft"=flimfft, "freqs"=freqs))
  }
  
  # find the edge of the ACH peak
  peakstart <- which(scalfact*diff_ach <= thres & c(which(size(diff_ach)>1), diff(diff_ach)) < 0)[1]
  
  if (is_empty(peakstart) || is.na(peakstart)) {
    if (warnings) {
      sprintf("Central peak could not be determined")
    }
    phw <- 1
  } else {
    phw <- peakstart
  }
  
  # STEP 4: fft and spectral peak
  # take the positive lags up to w
  tmp <- ach_smooth[(ceil(length(ach)/2)+phw):length(ach_smooth)]
  if (length(tmp) < w) {
    ach_nopeak = rep(0, w)
    ach_nopeak[1:length(tmp)] = tmp
  } else {
    ach_nopeak <- tmp[1:w]
  }
  
  # get fft and peak within freq limit
  cfg_flim = flim
  output <- spectral_peak(fs=fs, flim=cfg_flim, taper=taper, fcor=fcor, data=ach_nopeak) # need to modify parameters
  peakfreqx <- output$freq
  freqs <- output$fxx
  flimfft <- output$corfft
  
  # use a specific frequency of interest
  if (!is_empty(fpeak)) {
    id <- which(abs(freqs-fpeak) == min(abs(freqs-fpeak)))
    peakfreqx <- freqs[id]
  }
  
  # get fft of whole freq range
  cfg_flim <- c(0, fs/2)
  output <- spectral_peak(fs=fs, flim=cfg_flim, taper=taper, fcor=fcor, data=ach_nopeak)
  freq <- output$freq
  freqstot <- output$fxx
  totfft <- output$corfft 
  
  # STEP 5: compute the score
  if (is_empty(peakfreqx) || is.nan(peakfreqx)) {
    oscore <- NA
    fosc <- NA
    flim <- list()
    flimfft <- list()
    freqs <- list()
  } else {
    oscore <- flimfft[freqs==peakfreqx] / mean(totfft, na.rm=TRUE)
    fosc <- peakfreqx
  }
   
  # skip plotting for now
  return(list("oscore" = oscore, "fosc" = fosc, "flim" = flim, "flimfft" = flimfft, "freqs" = freqs))
}

# ----- oscillationScoreStats -----
# 
# DESCRIPTION:
# Generates reference signals, either by shuffling or based on trend-fit of
# the original data,and computes O-scores for these reference signals.
# 
# INPUTS:
# - cfg: struct of settings
# - signal: continuous data trace
# 
# OUTPUTS:
# - Oscore_rp: Oscores of reference signals
# - fosc_rp: frequencies of references signals
# - signrep: reference signals
# - trendfit: trend that was fitted to original data
# 
# CONFIG:
# - nrep: number of repetitions (required)
# - fs: sampling rate of signal (required)
# - flim: frequency range of interest (required)
# - fpeak: peak frequency of original signal (required)
# - keep_trend: switch that determines whether trend is fitted to original
# - data, if false, data is shuffled (boolean, default = false)
# - trend_dist: type of distribution to fit to original data (required for
# - keep_trend)
# - trend_ddt: time step for generation of trend (default = double of fs)
# - trend_alpha: significance level of goodness-of-fit test of trend fit
#   (default = 0.05)
# - warnings: verbose switch ('on' or 'off', default = 'on')
# - plot: switch for plotting 1st reference trace (boolean; default = false)

oscillation_score_stats <- function(quantlim, fs, mincycles, smoothach, smoothwind, peakwind, taper, warnings, fcor, nrep, flim, keep_trend, trend_dist, trend_ddt, trend_alpha, plot, fpeak, signal) {
  if (nrep <= 0) stop("Invalid number of repetitions")
  if (fs <= 0) stop("Invalid sampling rate")
  if (!(is.vector(flim) && length(flim) == 2)) stop("Invalid frequency range of interest")
  if (missing(fpeak)) stop("No peak frequency specified")
  
  if (is.na(fpeak)) {
    warning("Peak frequency is NaN, shuffling skipped")
    oscore_rp <- NaN
    fosc_rp <- NaN
    signrep <- list()
    trendfit <- list()
    return(list("oscore_rp" = oscore_rp, "fosc_rp" = fosc_rp, "signrep" = signrep, "trendfit" = trendfit)) 
  } else {
    if (!(fpeak >= flim[1] && fpeak <= flim[2])) stop("Invalid peak frequency of interest: ",fpeak)
  }
  
  if (missing(keep_trend)) keep_trend <- FALSE 
  if (keep_trend) {
    if (missing(trend_dist)) stop("No trend distribution was specified")
    if (missing(trend_ddt)) trend_ddt <- 0.5/fs
    if (missing(trend_alpha)) trend_alpha <- 0.05
  }
  
  if (missing(warnings)) warnings = TRUE
  if (missing(plot)) plot = FALSE
  
  ## check signal
  if (sum(signal) < 3) {
    if (warnings) {
      warning("Oscillation score stats could not be computed")
      oscore_rp <- NaN
      fosc_rp <- NaN
      signrep <- list()
      trendfit <- list()
      return(list("oscore_rp" = oscore_rp, "fosc_rp" = fosc_rp, "signrep" = signrep, "trendfit" = trendfit)) 
    }
  }
  
  ## fit distribution
  
  # truncate trace
  tstart <- max(1, which(signal!=0)[1] - 1)
  trace <- which(signal!=0)
  tend <- min(length(signal), trace[length(trace)] + 1)
  signq <- signal[tstart:tend]
  
  # check for memory
  if (length(signal) > 30000) {
    signrep <- list()
  } else {
    signrep <- matrix(0, nrep, length(signal)) 
  }
  
  if (!missing(keep_trend) && keep_trend) {
    dists <- trend_dist
    
    pst <- rep(1, length(dists))
    
    for (d in 1:length(dists)) {
      # fit distribution
      scale <- 10
      pd <- fitdistrplus::fitdist(which(signq!=0)/scale, dists[d]) # need to scale down for convergence
      
      #pd <- try(fitdistrplus::fitdist(which(signq!=0), dists[d]), silent=TRUE)
      if (typeof(pd)!="list") { # i.e. it didn't converge
        chi_test_results <- NaN
        pst[d] <- NaN
      } else {
        # test fit
        chi_test_results <- fitdistrplus::gofstat(pd)
        
        pst[d] <- chi_test_results$chisqpvalue
      }
    }
    
    # quality checks
    if (is.na(pst) == length(dists)) { # all p-vals are nan
       if (warnings) {
         warning("Cannot fit distribution, shuffling data instead.")
       }
      keep_trend <- FALSE
      
      trendfit <- list("distribution" = "Shuffle", "pd" = list(), "pval" = NaN, "stats" = list(), "trace"=list())
      
    } else {
      
      # test the fits
      ids <- which(pst>=trend_alpha)
      # the null hypothesis for the GOF test is that the data come from 
      # the fitted distribution, so we would prefer this not to be
      # rejected
      if (length(ids)==0 && warnings) {
        warning("No suitable distribution found, using the best option.")
      }
      
      # determine the winning distribution
      if (length(dists) > 1) {
        id <- max(pst)
      } else {
        id <- 1
      }
      
      # make a trace of the winning trace
      scale <- 10
      pd <- fitdistrplus::fitdist(which(signq!=0)/scale, dists[d]) # need to scale down for convergence
      # don't scale down
      #pd <- fitdistrplus::fitdist(which(signq!=0), dists[d])
      # test fit
      chi_test_results <- fitdistrplus::gofstat(pd)
      env <- stats::dgamma(1:length(signq), shape=pd$estimate["shape"], rate = pd$estimate["rate"]/scale)
      
      # store fit info
      trace <- rep(0, length(signal))
      trace[tstart:tend] <- env/sum(env)
      
      trendfit <- list("distribution" = dists[id], "pd" = pd, "pval" = chi_test_results$chisqpvalue, "stats" = chi_test_results, "trace"=trace)
    }
  } else {
    trendfit <- list("distribution" = "Shuffle", "pd" = list(), "pval" = NaN, "stats" = list(), "trace"=list())
  }
  
  ## generate reference data and compute Oscore
  oscore_rp <- rep(NaN, nrep)
  fosc_rp <- rep(NaN, nrep)
  signrep <- matrix(0, nrep, length(signal))
  
  for (rp in 1:nrep) {
    if (trendfit$distribution == "Shuffle"){
      # create reference traces by shuffling time stamps
      signrp <- rep(0, length(signq))
      for (i in which(signq!=0)) {
        if (is.na(fpeak)) {
          wind <- fs/flim[1]
        } else {
          wind <- fs/fpeak
        }
        j <- max(1, min(length(signrp), round(runif(1)*wind - wind/2) + i))
        signrp[j] <- signrp[j] + 1
      }
    } else {
      timeline <- seq(from = tstart/fs, to = tend/fs, by = trend_ddt)
      fact <- fs*trend_ddt
      #envfull <- stats::pnorm(pd, seq(from = 1, to = length(signq), by = fact))
      # the following line scales the distribution based on the rate
      envfull <- stats::dgamma(seq(from = 1, to = length(signq), by = fact), shape=pd$estimate["shape"], rate = pd$estimate["rate"]/scale) 
      # TO DO: throw error if shape specified is not gamma
      
      # generate button press time stamps based on trend
      signrpfull <- rep(0, length(timeline))
      matsize <- size(envfull,1) * size(envfull, 2)
      signrpfull[runif(matsize) < (sum(signal) * envfull / sum(envfull))] = 1
      
      # reduce the time step
      mct_sd_smooth <- list()
      mct_dt <- 1/fs
      mct_remove_val <- 0
      mct_warnings <- FALSE
      output <- make_continuous_trace(data = timeline[which(signrpfull!=0)], sd_smooth = mct_sd_smooth, dt = mct_dt, remove_val = mct_remove_val, warnings = mct_warnings)
      signd <- output$signal
      timed <- output$tspan
      
      signrp <- rep(0, length(signq))
      if (length(signd) > length(signrp)) {
        signrp <- signd[length(signd)-length(signrp)+1:length(signd)]
      } else {
        signrp[1:length(signd)] = signd
      }
    }
    
    signdum <- rep(0, length(signal))
    signdum[tstart:tend] <- signrp
    if (!isempty(signrep)) {
      signrep[rp,] <- signdum # store the reference traces
    }
    
    # compute oscore for reference trace
    output <- oscillation_score(signal = signdum, quantlim = quantlim, fs = fs, mincycles = mincycles, smoothach = smoothach, smoothwind = smoothwind, peakwind = peakwind, taper = taper, fcor = fcor, cfg_flim = flim, plot = plot, fpeak = fpeak)
    oscore_rp[rp] <- output$oscore
    fosc_rp[rp] <- output$fosc
    
    # skip plot for now
  }
    
  return(list("oscore_rp" = oscore_rp, "fosc_rp" = fosc_rp, "signrep" = signrep, "trendfit" = trendfit)) 
}

```

#### Compute O-scores

```{r compute_o_scores, echo=T}

# ----- compute_oscores_all_subs -----
#
# Compute an O-score for every participant and trial type in the data input
# INPUTS:
# - data: dataframe containing the columns trial_type, subjectId, acc (accuracy: 1/0) , rt (reaction time in seconds)
#
# OUTPUTS:
# - oscorez_correct_dataframe: dataframe containing the columns oscore_correct (raw O-score), oscorez_correct (z-scored O-score), fosc (frequency of the oscillation), and trial_type

compute_oscores_all_subs <- function(data) {
  dt = 0.001
  
  all_oscore_correct <- list()
  all_oscorez_correct <- list()
  all_fosc <- list()
  subids <- list()
  trials <- list()
  
  for (ttype in unique(data$trial_type)) {
    trial_type_data <- data %>%
        filter(trial_type == ttype)
    
    uq_subids <- unique(trial_type_data$subjectId)
    
    for (i in 1:length(uq_subids)) {
      subid <- uq_subids[i]
      
      sub_data <- data %>% 
                filter(subjectId == subid, trial_type == ttype) 
      
      summarized_data <- sub_data %>%
                group_by(acc) %>%
                summarise(n_trials = sum(acc, na.rm=TRUE))
      
      N_cor <- summarized_data %>%
                filter(acc == 1) %>%
                select(n_trials)
      if (nrow(N_cor) == 0) N_cor = 0 else N_cor = as.integer(N_cor)
      N_incor <- summarized_data %>%
                filter(acc == 0) %>%
                select(n_trials)
      if (nrow(N_incor) == 0) N_incor = 0 else N_incor = as.integer(N_incor)
      
      correct_RTs <- sub_data %>%
                        filter(!is.na(rt), acc==1)
      correct_RTs <- correct_RTs$rt
      
      sprintf('%s %s', N_cor, N_incor)
      
      if (N_cor < stats::qbinom(0.95, N_cor + N_incor, 1/2)) {
        print(paste('Subject', subid, 'excluded for bad performance'))
        next 
      } else if (length(correct_RTs) < 10){
        print(paste('Subject', subid, 'excluded due to too few data points'))
        next
      }
      
      output <- make_continuous_trace(data=correct_RTs, dt=dt, sd_smooth=list(), remove_val=0)
      sign_correct <- output$signal
      tspan_correct <- output$tspan
      
      output <- oscillation_score(
        fs=1000, 
        cfg_flim=c(0.5, 40), 
        signal = sign_correct, 
        mincycles = 3,
        quantlim = c(0.05, 0.95), 
        smoothach = TRUE, 
        smoothwind = 0.002, 
        peakwind = 0.008, 
        thresangle = 10, 
        taper='hanning', 
        fcor=FALSE,
        fpeak=list())
      
      oscore_correct <- output$oscore
      fosc_correct <- output$fosc
      
      # store raw oscore
      all_oscore_correct <- append(all_oscore_correct, oscore_correct)
      all_fosc <- append(all_fosc, fosc_correct)
      
      # o-score stats
      output <- oscillation_score_stats(
        fs=1000, 
        flim=c(0.5, 40), 
        signal = sign_correct, 
        mincycles = 3,
        quantlim = c(0.05, 0.95), 
        smoothach = TRUE, 
        smoothwind = 0.002, 
        peakwind = 0.008, 
        taper='hanning', 
        fcor=FALSE,
        keep_trend = 1,
        trend_dist = c('gamma'),
        trend_ddt = 0.0005,
        trend_alpha = 0.05,
        nrep = 500,
        warnings = FALSE,
        fpeak=fosc_correct) 
      
      oscore_correct_rep <- output$oscore_rp
      fosc_correct_rep <- output$fosc_rp
      trendfit <- output$trendfit
      
      # compute z-score
      all_oscorez_correct <- append(all_oscorez_correct, (log(oscore_correct) - mean(log(oscore_correct_rep), na.rm=TRUE)) / std(log(oscore_correct_rep)))
      trials <- append(trials, ttype)
      subids <- append(subids, subid)
    }
  }
  
  oscorez_correct_dataframe <- data.frame(
    subid = as.character(c(subids)),
    oscore_correct = as.double(c(all_oscore_correct)),
    oscorez_correct = as.double(c(all_oscorez_correct)),
    fosc = as.double(c(all_fosc)),
    trial_type = as.character(c(trials))
  )
  
  return(oscorez_correct_dataframe)
}

```

#### Compute O-scores

Compute O-scores for data from Experiment 1 and Experiment 5 in ter Wal et al., 2021

```{r process_orig_data, echo=T, results='hide'}

# I retrieved the behavioral data for Experiment 5 and Experiment 1 from the 
# following link and converted the .mat files to .csv: 
# https://figshare.com/collections/Data_for_Theta_rhythmicity_governs_the_timing_of_behavioural_and_hippocampal_responses_in_humans_specifically_during_memory-dependent_tasks/5192567
# NOTE: Since the original .mat files are publicly available on figshare, I am 
# not providing these csv's in my GitHub respository
data_mem <- read.csv("BehavioralData_Experiment5.csv")
data_vis <- read.csv("BehavioralData_Experiment1.csv")
data_orig <- rbind(data_mem, data_vis)

data_orig <- data_orig %>% 
          filter(!is.na(RTs)) %>%
          rename(acc = Acc, rt = RTs, subjectId = SubjID, trial_type = TrialType) %>%
          mutate(trial_type = recode(trial_type, "ret&catch" = "retrieval"))

time_series_data <- data_orig %>%
    group_by(trial_type, subjectId) %>%
    mutate(firstRT = first(rt), lastRT = last(rt)) %>%
    mutate(time_series_length = abs(lastRT - firstRT)) %>%
    ungroup() %>%
    select(subjectId, trial_type, time_series_length) %>%
    rename(subid = subjectId) %>%
    mutate(subid = as.character(subid)) %>%
    group_by(trial_type, subid) %>%
    summarise(time_series_length = mean(time_series_length))

oscorez_correct_dataframe_orig <- compute_oscores_all_subs(data_orig)

save(oscorez_correct_dataframe_orig, file = "oscillation_score.RData")
#load("oscillation_score.RData")

oscorez_correct_dataframe_orig <- oscorez_correct_dataframe_orig %>%
  filter(!is.na(oscorez_correct))

```

Compute O-scores for replication data in the current study

```{r process_replication_data, echo=T, results='hide'}

data_mem <- 
    list.files(path = "./wal_online_task", pattern = "*_MemoryResults.csv", recursive=T, full.names=T) %>% 
    map_df(~fread(.))
data_mem <- data_mem %>%
  filter(trial_type == "encoding-trial" | trial_type == "retrieval-trial") %>%
  filter(!grepl("/for_instructions/", associate)) %>% # remove practice trials
  filter(response != "arrowdown") %>% # remove "forgot" trials
  select(trial_type, rt, cue, associate, accurate, instruction, subjectId) %>%
  mutate(accurate = as.integer(as.logical(accurate))) %>%
  # I realized that it was unclear to me how accuracy in the encoding task is determined.
  # Participants' memory for each association was tested twice (semantic and perceptual questions).
  # I labeled encoding trials as accurate if participants were correct in at least one of
  # the subsequent memory retrieval trials, as this would indicate that participants successfully
  # retained some information about the association. However, it is possible that the original 
  # authors used a different coding scheme. 
  group_by(subjectId, associate) %>%
  mutate(accurate = max(accurate, na.rm=T)) %>% # assign retrieval accuracy to encoding trials
  ungroup() %>%
  select(trial_type, rt, accurate, subjectId) %>%
  rename(acc = accurate)
  
data_vis <- 
    list.files(path = "./wal_online_task", pattern = "*_VisualResults.csv", recursive=T, full.names=T) %>% 
    map_df(~fread(.))
data_vis <- data_vis %>%
  filter(trial_type == "visual-trial") %>% 
  mutate(accurate = as.integer(as.logical(accurate))) %>%
  select(trial_type, rt, accurate, subjectId) %>%
  rename(acc = accurate)

data_repl <- rbind(data_mem, data_vis)

data_repl <- data_repl %>% 
          filter(!is.na(rt)) %>%
          mutate(rt = rt/1000) %>% # convert ms to seconds 
          mutate(trial_type = recode(trial_type, "encoding-trial" = "encoding", "retrieval-trial" = "retrieval", "visual-trial" = "visual")) %>%
          filter(rt > mean(rt)-2.5*sd(rt) & rt < mean(rt)+2.5*sd(rt)) # remove trials where RT is ±2.5 SD away from mean
          # NOTE: In the original matlab analysis scripts, I did not find code that excluded trials in which RT was ±2.5 SD away from the mean. I therefore assumed that the data provided was already somewhat pre-processed and thus did not include this filtering procedure in the data reproduction cells above.

time_series_data_repl <- data_repl %>%
    group_by(trial_type, subjectId) %>%
    mutate(firstRT = first(rt), lastRT = last(rt)) %>%
    mutate(time_series_length = abs(lastRT - firstRT)) %>%
    ungroup() %>%
    select(subjectId, trial_type, time_series_length) %>%
    rename(subid = subjectId) %>%
    mutate(subid = as.character(subid)) %>%
    group_by(trial_type, subid) %>%
    summarise(time_series_length = mean(time_series_length))

oscorez_correct_dataframe_repl <- compute_oscores_all_subs(data_repl)

save(oscorez_correct_dataframe_repl, file = "oscillation_score_repl.RData")
#load("oscillation_score_repl.RData")

oscorez_correct_dataframe_repl <- oscorez_correct_dataframe_repl %>%
  filter(!is.na(oscorez_correct))

```

### Confirmatory analysis

As I specified in the analysis plan, I will assess whether the oscillation scores (z-scored O-scores) differ significantly between the memory-dependent and memory-independent tasks. The linear mixed effects model, as specified above, will be the main analysis of interest (I may just run a simple linear model if the mixed effects model does not converge). I will also performed paired t-tests to determine whether the O-scores are significantly above chance.

#### Recreation of Fig. 3A with data from two of the original experiments

```{r plot_oscore_orig}
ggplot(oscorez_correct_dataframe_orig, aes(trial_type, oscorez_correct)) +
  geom_boxplot(width=0.1, fill="dodgerblue2", color="gray27", outlier.shape = NA) +
  geom_jitter(width=0.1, fill="dodgerblue2", color="gray27", shape=21) +
  xlab("") +
  ylab("Oscillation score (Z-score)") + 
  ylim(0.5, 2.5) +
  ggtitle("Data from original experiments") + 
  theme_classic()
```

```{r plot_oscore_repl}
ggplot(oscorez_correct_dataframe_repl, aes(trial_type, oscorez_correct)) +
  geom_boxplot(width=0.1, fill="mediumpurple", color="gray27", outlier.shape = NA) +
  geom_jitter(width=0.1, fill="mediumpurple", color="gray27", shape=21) +
  xlab("") +
  ylab("Oscillation score (Z-score)") +
  ylim(0.5, 2.5) +
  ggtitle("Data from replication experiments") +
  theme_classic()
```

#### Linear model testing the effect of memory dependence on O-scores for the reproduced data

```{r lm_orig}
oscore_data <- merge(oscorez_correct_dataframe_orig, time_series_data, all.x=T)

oscore_data <- oscore_data %>%
  dplyr::mutate(memory_dependence = recode(trial_type, "encoding"=1, "retrieval"=1, "visual"=0)) %>%
  dplyr::filter(!is.na(oscorez_correct))

mod_lm_orig <- lm(oscorez_correct ~ memory_dependence + time_series_length, data=oscore_data)
print(summary(mod_lm_orig))
print(partial_f2(mod_lm_orig))
# g*2 suggests a sample size of ~50 for an f2 of 0.15
# it should be noted that because of the shuffling procedure used to standardize
# the O-scores, the f2 printed above may be slightly different
```

T-test comparing O-scores in the memory-dependent and memory-independent (visual) tasks

```{r vis_nonvis_ttest_orig}
vis_v_nonvis_ttest <- stats::t.test(
  oscorez_correct_dataframe_orig %>% 
    filter(trial_type != "visual") %>% 
    select(oscorez_correct), 
  oscorez_correct_dataframe_orig %>% 
    filter(trial_type == "visual") %>% 
    select(oscorez_correct))
report_statistics(vis_v_nonvis_ttest)
delta <- vis_v_nonvis_ttest$estimate["mean of x"] - vis_v_nonvis_ttest$estimate["mean of y"]
delta <- delta / sd(oscorez_correct_dataframe_orig$oscorez_correct)
print(paste("effect size:", round(delta, digits=2)))
print("g*power suggests a sample of 50 (difference btwn two independent means)")
```

T-test comparing O-scores in the encoding and visual tasks

```{r enc_vis_ttest_orig}
enc_v_vis_ttest <- stats::t.test(
  oscorez_correct_dataframe_orig %>% 
    filter(trial_type == "encoding") %>% 
    select(oscorez_correct), 
  oscorez_correct_dataframe_orig %>% 
    filter(trial_type == "visual") %>% 
    select(oscorez_correct))
report_statistics(enc_v_vis_ttest)
delta <- enc_v_vis_ttest$estimate["mean of x"] - enc_v_vis_ttest$estimate["mean of y"]
delta <- delta / sd(oscorez_correct_dataframe_orig$oscorez_correct)
print(paste("effect size:", round(delta, digits=2)))
```

T-test comparing O-scores in the retrieval and visual tasks

```{r ret_vis_ttest_orig}
ret_v_vis_ttest <- stats::t.test(
  oscorez_correct_dataframe_orig %>% 
    filter(trial_type == "retrieval") %>% 
    select(oscorez_correct), 
  oscorez_correct_dataframe_orig %>% 
    filter(trial_type == "visual") %>% 
    select(oscorez_correct))
report_statistics(ret_v_vis_ttest)
delta <- ret_v_vis_ttest$estimate["mean of x"] - ret_v_vis_ttest$estimate["mean of y"]
delta <- delta / sd(oscorez_correct_dataframe_orig$oscorez_correct)
print(paste("effect size:", round(delta, digits=2)))
```

T-test comparing O-scores in the encoding and retrieval tasks

```{r enc_ret_ttest_orig}
env_v_ret_ttest <- stats::t.test(
  oscorez_correct_dataframe_orig %>% 
    filter(trial_type == "encoding") %>% 
    select(oscorez_correct), 
  oscorez_correct_dataframe_orig %>% 
    filter(trial_type == "retrieval") %>% 
    select(oscorez_correct))
report_statistics(env_v_ret_ttest)
delta <- env_v_ret_ttest$estimate["mean of x"] - env_v_ret_ttest$estimate["mean of y"]
delta <- delta / sd(oscorez_correct_dataframe_orig$oscorez_correct)
print(paste("effect size:", round(delta, digits=2)))
```

#### Linear model testing the effect of memory dependence on O-scores for the replicated data

```{r lm_repl}
oscore_data <- merge(oscorez_correct_dataframe_repl, time_series_data_repl, all.x=T)

oscore_data <- oscore_data %>%
  dplyr::mutate(memory_dependence = recode(trial_type, "encoding"=1, "retrieval"=1, "visual"=0)) %>%
  dplyr::filter(!is.na(oscorez_correct))

mod_lm_repl <- lm(oscorez_correct ~ memory_dependence + time_series_length, data=oscore_data)
print(summary(mod_lm_repl))
print(partial_f2(mod_lm_repl))
```

T-test comparing O-scores in the memory-dependent and memory-independent (visual) tasks

```{r vis_nonvis_ttest_repl}
vis_v_nonvis_ttest <- stats::t.test(
  oscorez_correct_dataframe_repl %>%
    filter(trial_type != "visual") %>%
    select(oscorez_correct),
  oscorez_correct_dataframe_repl %>%
    filter(trial_type == "visual") %>%
    select(oscorez_correct))
report_statistics(vis_v_nonvis_ttest)
delta <- vis_v_nonvis_ttest$estimate["mean of x"] - vis_v_nonvis_ttest$estimate["mean of y"]
delta <- delta / sd(oscorez_correct_dataframe_repl$oscorez_correct)
print(paste("effect size:", round(delta, digits=2)))
```

T-test comparing O-scores in the encoding and visual tasks

```{r enc_vis_ttest_repl}
enc_v_vis_ttest <- stats::t.test(
  oscorez_correct_dataframe_repl %>%
    filter(trial_type == "encoding") %>%
    select(oscorez_correct),
  oscorez_correct_dataframe_repl %>%
    filter(trial_type == "visual") %>%
    select(oscorez_correct))
report_statistics(enc_v_vis_ttest)
delta <- enc_v_vis_ttest$estimate["mean of x"] - enc_v_vis_ttest$estimate["mean of y"]
delta <- delta / sd(oscorez_correct_dataframe_repl$oscorez_correct)
print(paste("effect size:", round(delta, digits=2)))
```

T-test comparing O-scores in the retrieval and visual tasks

```{r ret_vis_ttest_repl}
ret_v_vis_ttest <- stats::t.test(
  oscorez_correct_dataframe_repl %>%
    filter(trial_type == "retrieval") %>%
    select(oscorez_correct),
  oscorez_correct_dataframe_repl %>%
    filter(trial_type == "visual") %>%
    select(oscorez_correct))
report_statistics(ret_v_vis_ttest)
delta <- ret_v_vis_ttest$estimate["mean of x"] - ret_v_vis_ttest$estimate["mean of y"]
delta <- delta / sd(oscorez_correct_dataframe_repl$oscorez_correct)
print(paste("effect size:", round(delta, digits=2)))
```

T-test comparing O-scores in the encoding and retrieval tasks

```{r enc_ret_ttest_repl}
env_v_ret_ttest <- stats::t.test(
  oscorez_correct_dataframe_repl %>%
    filter(trial_type == "encoding") %>%
    select(oscorez_correct),
  oscorez_correct_dataframe_repl %>%
    filter(trial_type == "retrieval") %>%
    select(oscorez_correct))
report_statistics(env_v_ret_ttest)
delta <- env_v_ret_ttest$estimate["mean of x"] - env_v_ret_ttest$estimate["mean of y"]
delta <- delta / sd(oscorez_correct_dataframe_repl$oscorez_correct)
print(paste("effect size:", round(delta, digits=2)))
```

### Exploratory analyses

#### Behavioral performance

Experiments 1 and 5 in ter Wal et al., 2021

```{r behavioral_accuracy_orig}
kable(data_orig %>%
  group_by(subjectId, trial_type) %>%
  summarize(mean_accuracy = mean(acc)) %>%
  group_by(trial_type) %>%
  summarize(mean_accuracy = mean(mean_accuracy)),
  align='l')
```

Replication study

```{r behavioral_accuracy_repl}
kable(data_repl %>%
  group_by(subjectId, trial_type) %>%
  summarize(mean_accuracy = mean(acc)) %>%
  group_by(trial_type) %>%
  summarize(mean_accuracy = mean(mean_accuracy)),
  align='l')
```

#### Density plots of reaction times in the replication study

```{r response_density_encoding}
data_repl %>%
  filter(trial_type == "encoding") %>%
  mutate(acc = recode(acc, "1"="correct responses", "0"="incorrect responses")) %>%
  mutate(accuracy = acc) %>%
  ggplot(aes(rt, color=accuracy)) +
    geom_density() + 
    facet_wrap(~subjectId, ncol=5) +
    labs(x = "Time after object onset (s)") + 
    ggtitle("Encoding task") + 
    theme_minimal()
```

```{r response_density_retrieval}
data_repl %>%
  filter(trial_type == "retrieval") %>%
  mutate(acc = recode(acc, "1"="correct responses", "0"="incorrect responses")) %>%
  mutate(accuracy = acc) %>%
  ggplot(aes(rt, color=accuracy)) +
    geom_density() + 
    facet_wrap(~subjectId, ncol=5) +
    labs(x = "Time after retrieval cue onset (s)") + 
    ggtitle("Retrieval task") + 
    theme_minimal()
```

```{r response_density_visual}
data_repl %>%
  filter(trial_type == "visual") %>%
  mutate(acc = recode(acc, "1"="correct responses", "0"="incorrect responses")) %>%
  mutate(accuracy = acc) %>%
  ggplot(aes(rt, color=accuracy)) +
    geom_density() + 
    facet_wrap(~subjectId, ncol=5) +
    labs(x = "Time after stimulus onset (s)") + 
    ggtitle("Visual task") + 
    theme_minimal()
```

#### Recreation of Fig. 3C with data from two of the original experiments, looking at the peak frequency in the behavioral oscillations

```{r plot_freq_hist_orig}
ggplot(oscorez_correct_dataframe_orig, aes(y=fosc, after_stat(density))) + 
    geom_histogram(color="black", fill="dodgerblue2") + 
    geom_hline(yintercept=1, color="darkorange1",
      linetype="dashed") + 
    geom_hline(yintercept=5, color="darkorange1",
      linetype="dashed") + 
    ylab("Frequency (Hz)") + 
    xlab("Fraction of participants") +
    ggtitle("Data from original experiments") + 
    facet_wrap(~trial_type) + 
    theme_minimal()
```

```{r plot_freq_hist_repl}
ggplot(oscorez_correct_dataframe_repl, aes(y=fosc, after_stat(density))) +
    geom_histogram(color="black", fill="mediumpurple") +
    geom_hline(yintercept=1, color="darkorange1",
      linetype="dashed") +
    geom_hline(yintercept=5, color="darkorange1",
      linetype="dashed") +
    ylab("Frequency (Hz)") +
    xlab("Fraction of participants") +
    ggtitle("Data from replication experiments") +
    facet_wrap(~trial_type) +
    theme_minimal()
```

##### Does the peak frequency differ between tasks?

Data from original experiments 

```{r lm_fosc_orig}
oscore_data <- merge(oscorez_correct_dataframe_orig, time_series_data, all.x=T)

oscore_data <- oscore_data %>%
  mutate(memory_dependence = recode(trial_type, "encoding"=1, "retrieval"=1, "visual"=0)) %>%
  filter(!is.na(oscorez_correct))

mod_lm_fosc_orig <- lm(fosc ~ memory_dependence + time_series_length, data=oscore_data)
print(summary(mod_lm_fosc_orig))
```

Data from replication study

```{r lm_fosc_repl}
oscore_data <- merge(oscorez_correct_dataframe_repl, time_series_data_repl, all.x=T)

oscore_data <- oscore_data %>%
  mutate(memory_dependence = recode(trial_type, "encoding"=1, "retrieval"=1, "visual"=0)) %>%
  filter(!is.na(oscorez_correct))

mod_lm_fosc_repl <- lm(fosc ~ memory_dependence + time_series_length, data=oscore_data)
print(summary(mod_lm_fosc_repl))
```

These analyses suggest that the peak frequency was significantly lower in the memory-dependent compared to memory-independent tasks in the replication sample. This is consistent with the finding reported in the original paper; this effect was not statistically significant, however, in the two experiments for which I reproduced data. 

#### How do the O-scores and peak frequencies compare across samples?

```{r oscore_comparison_across_samples}
combined_data <- rbind(oscorez_correct_dataframe_orig %>%
                         mutate(sample="original"), 
                       oscorez_correct_dataframe_repl %>%
                         mutate(sample="replication"))

sample_mod_oscore <- lm(oscorez_correct ~ sample * trial_type, data=combined_data)
print(summary(sample_mod_oscore))

sample_mod_fosc <- lm(fosc ~ sample * trial_type, data=combined_data)
print(summary(sample_mod_fosc))
```

The O-scores and peak frequencies do not appear to differ significantly between the original in person sample and the online replication sample.

```{r test, include=FALSE}
orig_lm_stats <- report_statistics(mod_lm_orig)[2]
repl_lm_stats <- report_statistics(mod_lm_repl)[2]

vis_v_nonvis_ttest_stats <- report_statistics(vis_v_nonvis_ttest)
enc_v_vis_ttest_stats <- report_statistics(enc_v_vis_ttest)
ret_v_vis_ttest_stats <- report_statistics(ret_v_vis_ttest)
```

## Discussion

### Summary of Replication Attempt

In this project, I attempted to replicate the primary behavioral finding in ter Wal et al. (2021), in which the authors showed that behavioral responses are more oscillatory in memory-dependent compared to memory-independent tasks. My primary confirmatory analysis was a linear model that assessed the effects of memory dependence and the length of the response time series on Oscillation scores (O-scores). I first reproduced the O-scores from two of the original experiments (out of 13). Here, O-scores were higher in memory-dependent tasks compared to the memory-independent task: (`r orig_lm_stats`). In the replication study conducted online, the effect of memory dependence was statistically non-significant, but trending in the same direction: (`r repl_lm_stats`).

### Commentary

Additional preregistered analyses included t-tests comparing the O-scores in each type of task. These t-tests also showed that there was not a statistically significant difference between the O-scores in the memory-dependent (encoding + retrieval) and memory-independent (visual) tasks: (`r vis_v_nonvis_ttest_stats`). Looking further at whether there were differences between the individual memory-dependent tasks and the memory-independent task, t-tests revealed that O-scores between the encoding and visual tasks did not differ: (`r enc_v_vis_ttest_stats`). There was, however, there was a statistically significant difference between the O-scores in the retrieval and visual tasks: (`r ret_v_vis_ttest_stats`). There are reasons to believe that the responses in the retrieval task were more reliable than those in the encoding task, suggesting that with an experimental design more optimized for online data collection, the primary finding might replicate in online samples.

Below, I elaborate on some challenges I faced in running this replication study online and list some differences between this replication and the original study. These discrepancies could potentially explain why the primary confirmatory analysis did not replicate in this online sample. I also provide suggestions for improving the current version of the online study.   

First, piloting revealed that it was potentially unclear that responses were required in the encoding task. I subsequently modified the instructions quiz in the memory-dependent study to emphasize to participants that they should press the space bar as soon as they formed a mental image between the verb cue and object they were shown. It appears that the majority of participants in the final sample paid attention to, and followed this part of the instructions; only 3 participants' encoding data were excluded for having too few trials. Since the number of instructions quiz attempts is logged in the data files, it is possible to assess whether the number of failed attempts (a measure of instructions comprehension) negatively predicts performance in each task. This finding could inform whether a pre-set threshold on this number could potentially be used in the future as an exclusion criterion. The feedback from piloting and the failure of some participants to provide responses in some blocks of the encoding tasks raise the concern that the reaction times in the encoding task may not be particularly reliable with respect to the timing of memory formation. A visual inspection of the graphs above suggests that the encoding O-scores may indeed be noisier in the replication sample than in the original sample.

Second, feedback on the retrieval task by a participant who dropped out of the study suggests that the structure of the retrieval trials may also have been confusing. This participant was responding during the instructions display rather than during the presentation of the retrieval cue, and was confused about why their performance was 0%. In all tasks, visually indicating that a response was received, or having a "too slow" display when participants fail to respond, could be helpful. Of note, online participants may be particularly sensitive to feedback on their performance. Since there was only feedback in the distractor and retrieval tasks, but not in the encoding tasks, participants may have been more vigilant during retrieval than during encoding. Reaction times could therefore be more reliable in the retrieval task. In the future, feedback on the percentage of trials participants responded successfully in could be added after each encoding phase to make the tasks more similar and perhaps equate participants' engagement across tasks. 

Third, I am not confident that I coded accuracy in the encoding task in the same way as the original authors. As preregistered, I coded encoding trials as accurate if the participant responded correctly in at least one of the corresponding retrieval trials (as described above, participants were tested on their memory for each association twice, once on a semantic property of the object and a second time on a perceptual property of the object, in random order). It is possible that the authors used a different coding scheme (e.g., trials were only accurate if participants were accurate on both of the subsequent memory retrieval trials). My failure to replicate the difference in O-scores between the encoding and visual tasks could potentially be attributed to this potential discrepancy in the coding scheme.  

Fourth, it should be noted that the mean age of participants in the current experiments was roughly ten years older than the mean age of participants in the original behavioral studies. Of note, the mean age in the current samples is similar to that of the participants who took part in the intracranial EEG studies in the original paper (who performed similar tasks). It is unclear if age differences are expected in the effects of interest, but I suspect not.

Fifth, as with any online sample, it is possible that participants were multitasking as they were engaging with the tasks. The browser interaction data collected in this study could be used to assess how attentive each participant was. 

Sixth, the memory-dependent study was at least 3x as long as the memory-independent study. It is unclear if fatigue in the memory-dependent study could have introduced more variability in response times, subsequently impacting the oscillation metrics. 

Finally, there are a couple more differences between the original study and the online replication version that I would like to note: (1) The distractor task between the encoding and retrieval blocks was shorted in the current study, and participants could also decide when they wanted to start the distractor or the retrieval tasks. This variability in the duration of the delay between encoding and retrieval could have impacted memory performance. (2) In the memory-independent task, each object was only seen once, as opposed to twice in the original experiment. It is unlikely that this influenced the results.

In summary, more steps could have been taken to better optimize this replication study for an online format. Although the confirmatory analysis did not replicate, the statistically significant difference in Oscillation scores between the retrieval and visual tasks provides additional evidence that the timing of successful recall is oscillatory. This result demonstrates that it is possible to probe via online experimentation whether behavior in memory-dependent and memory-independent tasks is rhythmic. 
